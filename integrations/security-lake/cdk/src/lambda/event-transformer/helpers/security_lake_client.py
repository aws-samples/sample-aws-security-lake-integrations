#!/usr/bin/env python3
"""
Security Lake Client Helper

This module handles the sending of OCSF events to AWS Security Lake in Parquet format.
OCSF events are generated by the template system.
"""

import json
import logging
import os
import io
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
from uuid import uuid4

import boto3
from botocore.exceptions import ClientError

# Import numpy for array handling
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

# Parquet support for Security Lake compliance using PyArrow
# Using PyArrow's JSON reader (not from_pylist) to avoid element-wrapping bug
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
    logger.info(f"[PYARROW IMPORT] SUCCESS: pyarrow version {pa.__version__} loaded successfully")
except ImportError as e:
    PARQUET_AVAILABLE = False
    pa = None
    pq = None
    logger.error(f"[PYARROW IMPORT] FAILED: {str(e)}")
    quit()
except Exception as e:
    PARQUET_AVAILABLE = False
    pa = None
    pq = None
    logger.error(f"[PYARROW IMPORT] UNEXPECTED ERROR: {str(e)}")
    quit()



class SecurityLakeClient:
    """
    Security Lake client for sending template-generated OCSF events as Parquet files using PyArrow
    """
    
    def __init__(self, s3_bucket: str, source_configurations: List[Dict[str, Any]], s3_path: str = ''):
        """
        Initialize Security Lake client
        
        Args:
            s3_bucket: Security Lake S3 bucket name
            source_configurations: List of OCSF event class configurations
            s3_path: Optional S3 path prefix for OCSF files (e.g., 'ext/securitylakeb3fc896a/1.0/')
        """
        self.s3_bucket = s3_bucket
        self.source_configurations = source_configurations
        self.s3_path = s3_path.strip('/') if s3_path else ''  # Remove leading/trailing slashes
        self.s3_client = boto3.client('s3', config=boto3.session.Config(signature_version='s3v4'))
        
        logger.info(f"Initialized Security Lake client for bucket: {s3_bucket}")
        logger.info(f"S3 path prefix: {self.s3_path or '(root)'}")
        logger.info(f"Using PyArrow JSON reader for Parquet generation (avoids from_pylist element-wrapping bug)")
        
    def validate_configuration(self) -> bool:
        """
        Validate Security Lake client configuration
        
        Returns:
            True if configuration is valid, False otherwise
        """
        try:
            # Validate bucket exists and is accessible
            self.s3_client.head_bucket(Bucket=self.s3_bucket)
            
            # Validate source configurations
            if not self.source_configurations:
                logger.error("No source configurations provided")
                return False
            
            # Validate each source configuration has required fields
            # Config format: {'sourceName': '...', 'sourceVersion': '...' (eventClasses optional)}
            for config in self.source_configurations:
                if 'sourceName' not in config:
                    logger.error(f"Invalid source configuration (missing sourceName): {config}")
                    return False
            
            logger.info("Security Lake client configuration is valid")
            return True
            
        except ClientError as e:
            logger.error(f"Failed to validate Security Lake configuration: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error validating configuration: {str(e)}")
            return False
    
    def send_events_to_security_lake(self, events: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Send template-generated OCSF events to Security Lake, grouped by event class
        
        Args:
            events: List of template-generated OCSF events (can be mixed event classes)
            
        Returns:
            Dictionary with success/failure counts
        """
        if not events:
            logger.warning("No events provided to send to Security Lake")
            return {'successful': 0, 'failed': 0}
        
        # Group events by class_name
        events_by_class = {}
        for event in events:
            class_name = event.get('class_name', 'Unknown')
            if class_name not in events_by_class:
                events_by_class[class_name] = []
            events_by_class[class_name].append(event)
        
        # Send each group
        total_successful = 0
        total_failed = 0
        
        for event_class, class_events in events_by_class.items():
            result = self.send_ocsf_events(class_events, event_class)
            total_successful += result['successful']
            total_failed += result['failed']
        
        return {'successful': total_successful, 'failed': total_failed}
    
    def send_ocsf_events(self, events: List[Dict[str, Any]], event_class: str) -> Dict[str, int]:
        """
        Send template-generated OCSF events to Security Lake as Parquet using PyArrow
        
        Args:
            events: List of template-generated OCSF events (all same event class)
            event_class: OCSF event class name (e.g., 'Compliance Finding')
            
        Returns:
            Dictionary with success/failure counts
        """
        if not events:
            logger.warning("No events provided to send to Security Lake")
            return {'successful': 0, 'failed': 0}
            
        if not PARQUET_AVAILABLE:
            logger.error("Parquet format not available - cannot send events to Security Lake")
            return {'successful': 0, 'failed': len(events)}
        
        try:
            # Detect source type from event data to use correct prefix
            if not self.source_configurations:
                logger.error(f"No source configurations available for event class: {event_class}")
                return {'successful': 0, 'failed': len(events)}
            
            # Check if these are GCP events (by checking cloud provider)
            first_event = events[0] if events else {}
            cloud_provider = first_event.get('cloud', {}).get('provider', '')
            
            # Use appropriate source name based on cloud provider
            if cloud_provider == 'GCP':
                source_name = 'gcp-scc'
                logger.debug(f"Detected GCP event, using source name 'gcp-scc'")
            else:
                # Default to first configuration (Azure/Defender events)
                source_name = self.source_configurations[0].get('sourceName', 'defenderIntegration')
                logger.debug(f"Using source name '{source_name}' for event class '{event_class}'")
            
            # Extract account ID from OCSF event for accountId partition
            # For Azure: cloud.account.uid contains Azure subscription ID
            # For GCP: cloud.account.uid contains GCP project_id (e.g., "cs-host-1d286108d2004c58af27ba")
            first_event = events[0] if events else {}
            account_id = first_event.get('cloud', {}).get('account', {}).get('uid', 'unknown')
            
            # Extract region from event data
            # For GCP: cloud.region contains GCP region from resource.labels.location
            # For Azure: Use AWS Lambda's running region (Azure doesn't provide region in flow logs)
            if cloud_provider == 'GCP':
                region = first_event.get('cloud', {}).get('region', 'unknown')
                logger.debug(f"Using GCP region from event: {region}")
            else:
                # For Azure events, use AWS Lambda's running region
                region = os.environ.get('AWS_REGION', 'us-east-1')
                logger.debug(f"Using AWS Lambda region for Azure event: {region}")
            
            logger.info(f"Partitioning for {cloud_provider} event: region={region}, account_id={account_id}")
            
            # Generate S3 key with partitioning
            timestamp = datetime.now(timezone.utc)
            s3_key = self._generate_s3_key(source_name, event_class, timestamp, region, account_id)
            
            # Create Parquet buffer using PyArrow
            parquet_buffer = self._create_parquet_buffer(events, event_class)
            
            # Upload Parquet file to S3
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=s3_key,
                Body=parquet_buffer,
                ContentType='application/octet-stream',
                Metadata={
                    'source': source_name,
                    'event_class': event_class,
                    'event_count': str(len(events)),
                    'ocsf_version': '1.1.0',
                    'format': 'parquet',
                    'compression': 'snappy',
                    'created_by': 'microsoft-defender-integration',
                    'parquet_engine': 'pyarrow-json-reader'
                }
            )
            
            logger.info(f"Successfully sent {len(events)} {event_class} events to Security Lake as Parquet at {s3_key}")
            
            return {'successful': len(events), 'failed': 0}
            
        except Exception as e:
            logger.error(f"Failed to create Parquet file for {event_class} events: {str(e)}")
            return {'successful': 0, 'failed': len(events)}
    
    def _create_parquet_buffer(self, events: List[Dict[str, Any]], event_class: str) -> bytes:
        """
        Create Apache Parquet buffer using PyArrow's from_pylist method
        
        Uses PyArrow's Table.from_pylist() to convert cleaned Python dictionaries
        directly to Parquet format with proper nested structures for OCSF events.
        Events are cleaned to remove empty dicts/lists and normalize field names
        before conversion to ensure Parquet compatibility.
        
        Args:
            events: List of template-generated OCSF events (same event class)
            event_class: OCSF event class name
            
        Returns:
            Parquet file as bytes buffer
        """
        try:
            logger.debug(f"[PYARROW DEBUG] Starting _create_parquet_buffer with {len(events)} events for class: {event_class}")
            
            # Clean events to remove empty dicts/lists and problematic fields
            cleaned_events = [self._clean_event_for_pyarrow(event) for event in events]
            logger.debug(f"[PYARROW DEBUG] Cleaned {len(cleaned_events)} events")
            
            if pa is None or pq is None:
                raise Exception(f"PyArrow modules are None - PARQUET_AVAILABLE={PARQUET_AVAILABLE}")
            
            # Convert any numpy arrays back to plain Python lists
            # Ensures arrays are stored as simple lists, not numpy array types
            if NUMPY_AVAILABLE and np is not None:
                def denumpyify(obj):
                    """Recursively convert numpy arrays to Python lists"""
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, dict):
                        return {k: denumpyify(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [denumpyify(item) for item in obj]
                    else:
                        return obj
                
                cleaned_events = [denumpyify(event) for event in cleaned_events]
                logger.debug(f"[PYARROW DEBUG] Converted numpy arrays to Python lists")
            else:
                logger.debug(f"[PYARROW DEBUG] numpy not available, skipping array conversion")
            
            # Use PyArrow's from_pylist to create nested structure
            # This creates proper Parquet nested columns with:
            #   Column NAME = leaf field (e.g., "provider")
            #   Column PATH = full hierarchy (e.g., "cloud.provider")
            
            table = pa.Table.from_pylist(cleaned_events)
            
            logger.debug(f"[PYARROW DEBUG] Created PyArrow Table with {table.num_rows} rows and {table.num_columns} columns")
            logger.debug(f"[PYARROW DEBUG] Table will have nested column paths (e.g., cloud.provider)")
            
            # Create in-memory buffer
            buffer = io.BytesIO()
            
            # Write to Parquet with GZIP compression
            pq.write_table(
                table,
                buffer,
                compression='gzip',
                use_dictionary=True,
                write_statistics=True
            )
            
            # Get buffer contents
            parquet_data = buffer.getvalue()
            buffer.close()
            
            # Verify size constraints
            parquet_size_mb = len(parquet_data) / (1024 * 1024)
            if parquet_size_mb > 256:
                logger.warning(f"Parquet file size ({parquet_size_mb:.1f}MB) exceeds Security Lake 256MB recommendation")
            
            logger.debug(f"Created Parquet file using PyArrow JSON reader: {len(events)} events, {parquet_size_mb:.2f}MB, {event_class} class")
            
            return parquet_data
            
        except Exception as e:
            logger.error(f"Failed to create Parquet buffer with PyArrow: {str(e)}")
            logger.error(f"[PYARROW DEBUG] Exception type: {type(e).__name__}")
            import traceback
            logger.error(f"[PYARROW DEBUG] Traceback: {traceback.format_exc()}")
            raise
    
    def _clean_event_for_pyarrow(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Clean OCSF event to remove empty dicts/lists and fix Azure EventHub byte string keys.
        - Removes empty dicts/lists that cause issues
        - Cleans Azure EventHub metadata byte string keys like b'x-opt-enqueued-time'
        - Drops event_metadata field entirely (not needed in Security Lake)
        - Removes "$" and "@" from dictionary keys (invalid Parquet field names)
        
        Args:
            event: Template-generated OCSF event dictionary
            
        Returns:
            Cleaned event with empty structures removed and keys normalized
        """
        if isinstance(event, dict):
            cleaned = {}
            for key, value in event.items():
                # Drop event_metadata field entirely - not needed in Security Lake
                if key == 'event_metadata':
                    continue
                
                # Drop raw_data field entirely - causes OpenSearch mapping issues
                if key == 'raw_data':
                    continue
                
                # Clean up byte string keys from Azure EventHub metadata
                # Convert b'x-opt-enqueued-time' â†’ x-opt-enqueued-time
                clean_key = key
                if isinstance(key, str) and key.startswith("b'") and key.endswith("'"):
                    clean_key = key[2:-1]  # Remove b' prefix and ' suffix
                elif isinstance(key, str) and key.startswith('b"') and key.endswith('"'):
                    clean_key = key[2:-1]  # Remove b" prefix and " suffix
                
                # Sanitize keys: remove "$", "@", "(", ")" and replace spaces/hyphens with underscores
                # Valid Parquet field names: alphanumeric, underscore, dot only
                if isinstance(clean_key, str):
                    clean_key = (clean_key.replace('$', '')
                                         .replace('@', '')
                                         .replace('(', '')
                                         .replace(')', '')
                                         .replace(' ', '_')
                                         .replace('-', '_'))
                
                if isinstance(value, dict):
                    # Recursively clean nested dicts
                    cleaned_value = self._clean_event_for_pyarrow(value)
                    # Only include non-empty dicts
                    if cleaned_value:
                        cleaned[clean_key] = cleaned_value
                elif isinstance(value, list):
                    # For non-empty lists, check if they should be flattened to avoid PyArrow element-wrapping
                    if value:
                        # Check if this is a simple list of strings (flatten to comma-separated)
                        if all(isinstance(item, str) for item in value):
                            # Flatten to comma-separated string to avoid PyArrow's element-wrapping bug
                            cleaned[clean_key] = ','.join(value)
                        else:
                            # For lists of objects or mixed types, recursively clean but keep as list
                            cleaned_list = [self._clean_event_for_pyarrow(item) for item in value]
                            # Remove None values and empty dicts from list
                            cleaned_list = [item for item in cleaned_list if item not in (None, {}, [])]
                            if cleaned_list:
                                cleaned[clean_key] = cleaned_list
                elif value is not None:
                    # Include non-None primitive values
                    cleaned[clean_key] = value
            return cleaned
        elif isinstance(event, list):
            # Clean list items
            cleaned_list = [self._clean_event_for_pyarrow(item) for item in event]
            return [item for item in cleaned_list if item not in (None, {}, [])]
        else:
            # Return primitive values as-is
            return event
     
    def _generate_s3_key(self, source_name: str, event_class: str, timestamp: datetime,
                         region: str = 'Azure', account_id: str = 'unknown') -> str:
        """
        Generate S3 key with proper partitioning for Security Lake
        
        Format: {s3_path}/region={region}/accountid={accountid}/eventday=YYYYMMDD/{sourceName}{uuid}.parquet
        
        Args:
            source_name: Source name from configuration
            event_class: OCSF event class name (not used in path, kept for compatibility)
            timestamp: Event timestamp for partitioning
            region: AWS region or 'unknown'
            account_id: Account ID or 'unknown'
            
        Returns:
            Complete S3 key path
        """
        # Generate partition keys
        event_day = timestamp.strftime('%Y%m%d')
        filename = f"{source_name}{uuid4()}.parquet"
        
        # Build S3 key with Security Lake partitioning format
        # Format: ext/.../region=.../accountid=.../eventday=.../source=.../{file}
        parts = [
            self.s3_path,
            f"region={region}",
            f"accountid={account_id}",
            f"eventday={event_day}",
            filename
        ]
        
        # Filter out empty parts and join
        s3_key = '/'.join([p for p in parts if p])
        logger.info(f"Storing parquet file {s3_key}")
        return s3_key