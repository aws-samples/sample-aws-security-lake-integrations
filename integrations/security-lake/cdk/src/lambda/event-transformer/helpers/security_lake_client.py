#!/usr/bin/env python3
"""
Security Lake Client Helper

This module handles the sending of OCSF events to AWS Security Lake in Parquet format.
OCSF events are generated by the template system.
"""

import json
import logging
import os
import io
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
from uuid import uuid4

import boto3
from botocore.exceptions import ClientError

# Import numpy for array handling (awswrangler may use it internally)
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False

logger = logging.getLogger(__name__)

# Parquet support for Security Lake compliance using AWS Wrangler
# AWS Wrangler provides higher-level abstractions for AWS services
# and handles array serialization correctly when used with proper dtype settings
# AWS Wrangler provided by Lambda Layer (not in requirements.txt to avoid bundling)
try:
    import awswrangler as wr
    import pandas as pd
    PARQUET_AVAILABLE = True
    logger.info(f"[AWSWRANGLER IMPORT] SUCCESS: awswrangler loaded successfully")
except ImportError as e:
    PARQUET_AVAILABLE = False
    wr = None
    pd = None
    logger.error(f"[AWSWRANGLER IMPORT] FAILED: {str(e)}")
    quit()
except Exception as e:
    PARQUET_AVAILABLE = False
    wr = None
    pd = None
    logger.error(f"[AWSWRANGLER IMPORT] UNEXPECTED ERROR: {str(e)}")
    quit()



class SecurityLakeClient:
    """
    Security Lake client for sending template-generated OCSF events as Parquet files using PyArrow
    """
    
    def __init__(self, s3_bucket: str, source_configurations: List[Dict[str, Any]], s3_path: str = ''):
        """
        Initialize Security Lake client
        
        Args:
            s3_bucket: Security Lake S3 bucket name
            source_configurations: List of OCSF event class configurations
            s3_path: Optional S3 path prefix for OCSF files (e.g., 'ext/securitylakeb3fc896a/1.0/')
        """
        self.s3_bucket = s3_bucket
        self.source_configurations = source_configurations
        self.s3_path = s3_path.strip('/') if s3_path else ''  # Remove leading/trailing slashes
        self.s3_client = boto3.client('s3', config=boto3.session.Config(signature_version='s3v4'))
        
        logger.info(f"Initialized Security Lake client for bucket: {s3_bucket}")
        logger.info(f"S3 path prefix: {self.s3_path or '(root)'}")
        logger.info("Now using AWS Wrangler for Parquet generation (better AWS integration and array handling)")
        
    def validate_configuration(self) -> bool:
        """
        Validate Security Lake client configuration
        
        Returns:
            True if configuration is valid, False otherwise
        """
        try:
            # Validate bucket exists and is accessible
            self.s3_client.head_bucket(Bucket=self.s3_bucket)
            
            # Validate source configurations
            if not self.source_configurations:
                logger.error("No source configurations provided")
                return False
            
            # Validate each source configuration has required fields
            # Config format: {'sourceName': '...', 'sourceVersion': '...' (eventClasses optional)}
            for config in self.source_configurations:
                if 'sourceName' not in config:
                    logger.error(f"Invalid source configuration (missing sourceName): {config}")
                    return False
            
            logger.info("Security Lake client configuration is valid")
            return True
            
        except ClientError as e:
            logger.error(f"Failed to validate Security Lake configuration: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error validating configuration: {str(e)}")
            return False
    
    def send_events_to_security_lake(self, events: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Send template-generated OCSF events to Security Lake, grouped by event class
        
        Args:
            events: List of template-generated OCSF events (can be mixed event classes)
            
        Returns:
            Dictionary with success/failure counts
        """
        if not events:
            logger.warning("No events provided to send to Security Lake")
            return {'successful': 0, 'failed': 0}
        
        # Group events by class_name
        events_by_class = {}
        for event in events:
            class_name = event.get('class_name', 'Unknown')
            if class_name not in events_by_class:
                events_by_class[class_name] = []
            events_by_class[class_name].append(event)
        
        # Send each group
        total_successful = 0
        total_failed = 0
        
        for event_class, class_events in events_by_class.items():
            result = self.send_ocsf_events(class_events, event_class)
            total_successful += result['successful']
            total_failed += result['failed']
        
        return {'successful': total_successful, 'failed': total_failed}
    
    def send_ocsf_events(self, events: List[Dict[str, Any]], event_class: str) -> Dict[str, int]:
        """
        Send template-generated OCSF events to Security Lake as Parquet using PyArrow
        
        Args:
            events: List of template-generated OCSF events (all same event class)
            event_class: OCSF event class name (e.g., 'Compliance Finding')
            
        Returns:
            Dictionary with success/failure counts
        """
        if not events:
            logger.warning("No events provided to send to Security Lake")
            return {'successful': 0, 'failed': 0}
            
        if not PARQUET_AVAILABLE:
            logger.error("Parquet format not available - cannot send events to Security Lake")
            return {'successful': 0, 'failed': len(events)}
        
        try:
            # Detect source type from event data to use correct prefix
            if not self.source_configurations:
                logger.error(f"No source configurations available for event class: {event_class}")
                return {'successful': 0, 'failed': len(events)}
            
            # Check if these are GCP events (by checking cloud provider)
            first_event = events[0] if events else {}
            cloud_provider = first_event.get('cloud', {}).get('provider', '')
            
            # Use appropriate source name based on cloud provider
            if cloud_provider == 'GCP':
                source_name = 'gcp-scc'
                logger.debug(f"Detected GCP event, using source name 'gcp-scc'")
            else:
                # Default to first configuration (Azure/Defender events)
                source_name = self.source_configurations[0].get('sourceName', 'defenderIntegration')
                logger.debug(f"Using source name '{source_name}' for event class '{event_class}'")
            
            # Extract account ID from OCSF event for accountId partition
            # For Azure: cloud.account.uid contains Azure subscription ID
            # For GCP: cloud.account.uid contains GCP project_id (e.g., "cs-host-1d286108d2004c58af27ba")
            first_event = events[0] if events else {}
            account_id = first_event.get('cloud', {}).get('account', {}).get('uid', 'unknown')
            
            # Extract region from event data
            # For GCP: cloud.region contains GCP region from resource.labels.location
            # For Azure: Use AWS Lambda's running region (Azure doesn't provide region in flow logs)
            if cloud_provider == 'GCP':
                region = first_event.get('cloud', {}).get('region', 'unknown')
                logger.debug(f"Using GCP region from event: {region}")
            else:
                # For Azure events, use AWS Lambda's running region
                region = os.environ.get('AWS_REGION', 'us-east-1')
                logger.debug(f"Using AWS Lambda region for Azure event: {region}")
            
            logger.info(f"Partitioning for {cloud_provider} event: region={region}, account_id={account_id}")
            
            # Generate S3 key with partitioning
            timestamp = datetime.now(timezone.utc)
            s3_key = self._generate_s3_key(source_name, event_class, timestamp, region, account_id)
            
            # Write Parquet file directly to S3 using AWS Wrangler
            self._write_parquet_to_s3(events, event_class, s3_key, source_name)
            
            logger.info(f"Successfully sent {len(events)} {event_class} events to Security Lake as Parquet at {s3_key}")
            
            return {'successful': len(events), 'failed': 0}
            
        except Exception as e:
            logger.error(f"Failed to create Parquet file for {event_class} events: {str(e)}")
            return {'successful': 0, 'failed': len(events)}
    
    def _write_parquet_to_s3(self, events: List[Dict[str, Any]], event_class: str, s3_key: str, source_name: str) -> None:
        """
        Write events directly to S3 as Parquet using AWS Wrangler
        
        AWS Wrangler and pandas will handle array/list fields naturally, converting them
        to proper Parquet list types. No pre-serialization needed.
        
        Args:
            events: List of template-generated OCSF events (same event class)
            event_class: OCSF event class name
            s3_key: Full S3 key path for the Parquet file
            source_name: Source name for metadata
        """
        try:
            logger.debug(f"[AWSWRANGLER] Starting _write_parquet_to_s3 with {len(events)} events for class: {event_class}")
            
            # Clean events to remove empty dicts/lists and problematic fields
            cleaned_events = [self._clean_event_for_awswrangler(event) for event in events]
            logger.debug(f"[AWSWRANGLER] Cleaned {len(cleaned_events)} events")
            
            if wr is None or pd is None:
                raise Exception(f"AWS Wrangler modules are None - PARQUET_AVAILABLE={PARQUET_AVAILABLE}")
            
            # Convert events to pandas DataFrame
            # Arrays remain as Python lists - pandas/awswrangler will handle them naturally
            df = pd.DataFrame.from_records(cleaned_events)
            logger.debug(f"[AWSWRANGLER] Created pandas DataFrame with {len(df)} rows and {len(df.columns)} columns")
            
            # Remove columns with only empty structs (PyArrow cannot serialize empty struct types)
            df = self._remove_empty_struct_columns(df)
            logger.debug(f"[AWSWRANGLER] After removing empty structs: {len(df)} rows and {len(df.columns)} columns")
            
            # Build full S3 path
            s3_path = f"s3://{self.s3_bucket}/{s3_key}"
            
            # Write DataFrame to S3 as Parquet using AWS Wrangler
            # AWS Wrangler handles array serialization to proper Parquet list types
            wr.s3.to_parquet(
                df=df,
                path=s3_path,
                index=False,
                dataset=True,
                compression='snappy',
                boto3_session=None,  # Uses default session
                s3_additional_kwargs={
                    'Metadata': {
                        'source': source_name,
                        'event_class': event_class,
                        'event_count': str(len(events)),
                        'ocsf_version': '1.1.0',
                        'format': 'parquet',
                        'compression': 'snappy',
                        'created_by': 'EventTransformer',
                        'parquet_engine': 'awswrangler'  # Using AWS Wrangler for array handling
                    }
                }
            )
            
            logger.info(f"Wrote Parquet file using AWS Wrangler: {len(events)} events, {event_class} class to {s3_path}")
            
        except Exception as e:
            logger.error(f"Failed to write Parquet file with AWS Wrangler: {str(e)}")
            logger.error(f"[AWSWRANGLER] Exception type: {type(e).__name__}")
            import traceback
            logger.error(f"[AWSWRANGLER] Traceback: {traceback.format_exc()}")
            raise
    
    def _clean_event_for_awswrangler(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Clean OCSF event to remove empty dicts/lists and fix Azure EventHub byte string keys.
        - Removes empty dicts/lists that cause issues
        - Cleans Azure EventHub metadata byte string keys like b'x-opt-enqueued-time'
        - Drops event_metadata field entirely (not needed in Security Lake)
        - Removes "$" and "@" from dictionary keys (invalid Parquet field names)
        - Converts Python repr strings back to proper arrays (e.g., "['cloud']" -> ["cloud"])
        
        Args:
            event: Template-generated OCSF event dictionary
            
        Returns:
            Cleaned event with empty structures removed and keys normalized
        """
        import ast
        if isinstance(event, dict):
            cleaned = {}
            for key, value in event.items():
                # Drop event_metadata field entirely - not needed in Security Lake
                if key == 'event_metadata':
                    continue
                
                # Drop raw_data field entirely - causes OpenSearch mapping issues
                if key == 'raw_data':
                    continue
                
                # Clean up byte string keys from Azure EventHub metadata
                # Convert b'x-opt-enqueued-time' â†’ x-opt-enqueued-time
                clean_key = key
                if isinstance(key, str) and key.startswith("b'") and key.endswith("'"):
                    clean_key = key[2:-1]  # Remove b' prefix and ' suffix
                elif isinstance(key, str) and key.startswith('b"') and key.endswith('"'):
                    clean_key = key[2:-1]  # Remove b" prefix and " suffix
                
                # Sanitize keys: remove "$", "@", "(", ")" and replace spaces/hyphens with underscores
                # Valid Parquet field names: alphanumeric, underscore, dot only
                if isinstance(clean_key, str):
                    clean_key = (clean_key.replace('$', '')
                                         .replace('@', '')
                                         .replace('(', '')
                                         .replace(')', '')
                                         .replace(' ', '_')
                                         .replace('-', '_'))
                
                if isinstance(value, dict):
                    # Recursively clean nested dicts
                    cleaned_value = self._clean_event_for_awswrangler(value)
                    # Only include non-empty dicts
                    if cleaned_value:
                        cleaned[clean_key] = cleaned_value
                elif isinstance(value, list):
                    # Keep arrays as proper Python lists for PyArrow serialization
                    # PyArrow's Table.from_pylist() correctly handles nested lists and arrays
                    # OCSF schema REQUIRES array fields (e.g., compliance.standards, metadata.profiles)
                    # to remain as arrays, NOT flattened to comma-separated strings
                    if value:
                        # Check if this is a simple list of primitives (strings, numbers, etc.)
                        if all(isinstance(item, (str, int, float, bool)) for item in value):
                            # Keep as list - PyArrow will serialize correctly to Parquet list<string> type
                            cleaned[clean_key] = value
                        else:
                            # For lists of objects or mixed types, recursively clean but keep as list
                            cleaned_list = [self._clean_event_for_awswrangler(item) for item in value]
                            # Remove None values and empty dicts from list
                            cleaned_list = [item for item in cleaned_list if item not in (None, {}, [])]
                            if cleaned_list:
                                cleaned[clean_key] = cleaned_list
                elif value is not None:
                    # Check if this is a Python repr string that should be an array
                    # Common with template outputs: "['cloud']" or "[{'name': 'x'}]"
                    if isinstance(value, str) and value.startswith('[') and value.endswith(']'):
                        try:
                            # First try direct parsing
                            parsed_value = ast.literal_eval(value)
                            if isinstance(parsed_value, list):
                                # Recursively clean the parsed list
                                cleaned_list = [self._clean_event_for_awswrangler(item) for item in parsed_value]
                                cleaned_list = [item for item in cleaned_list if item not in (None, {}, [])]
                                if cleaned_list:
                                    cleaned[clean_key] = cleaned_list
                                else:
                                    # Empty list after cleaning, skip
                                    pass
                            else:
                                # Not a list after parsing, keep as string
                                cleaned[clean_key] = value
                        except (ValueError, SyntaxError):
                            # Malformed Python repr - try fixing by adding commas after dict/list closures
                            # Common issue: "[{...}\n {...}]" should be "[{...}, {...}]"
                            try:
                                import re
                                # Fix malformed list strings by adding commas after } or ] followed by whitespace and {
                                fixed_value = re.sub(r'(\}|\])\s+(\{|\[)', r'\1, \2', value)
                                parsed_value = ast.literal_eval(fixed_value)
                                if isinstance(parsed_value, list):
                                    # Recursively clean the parsed list
                                    cleaned_list = [self._clean_event_for_awswrangler(item) for item in parsed_value]
                                    cleaned_list = [item for item in cleaned_list if item not in (None, {}, [])]
                                    if cleaned_list:
                                        cleaned[clean_key] = cleaned_list
                                    else:
                                        pass
                                else:
                                    # Not a list, keep as string
                                    cleaned[clean_key] = value
                            except (ValueError, SyntaxError):
                                # Still can't parse, keep as string
                                cleaned[clean_key] = value
                    else:
                        # Include non-None primitive values
                        cleaned[clean_key] = value
            return cleaned
        elif isinstance(event, list):
            # Clean list items
            cleaned_list = [self._clean_event_for_awswrangler(item) for item in event]
            return [item for item in cleaned_list if item not in (None, {}, [])]
        else:
            # Return primitive values as-is
            return event
    
    def _remove_empty_struct_columns(self, df):
        """
        Remove DataFrame columns that contain only empty dictionaries/structs.
        PyArrow cannot serialize empty struct types to Parquet, so we must remove them.
        
        Args:
            df: Pandas DataFrame to clean
            
        Returns:
            DataFrame with empty struct columns removed
        """
        columns_to_drop = []
        
        logger.debug(f"[EMPTY_STRUCT_CHECK] Checking {len(df.columns)} columns for empty structs")
        
        for col in df.columns:
            # Check if all values in this column are empty dicts
            try:
                # Get all non-null values in this column
                non_null_values = df[col].dropna()
                
                logger.debug(f"[EMPTY_STRUCT_CHECK] Column '{col}': {len(non_null_values)} non-null values, dtype={df[col].dtype}")
                
                if len(non_null_values) == 0:
                    # Column is all nulls - can keep as-is
                    logger.debug(f"[EMPTY_STRUCT_CHECK] Column '{col}': All nulls, keeping")
                    continue
                
                # Sample first value for diagnosis
                first_val = non_null_values.iloc[0]
                logger.debug(f"[EMPTY_STRUCT_CHECK] Column '{col}': First value type={type(first_val)}, value={repr(first_val)}")
                
                # Check if all non-null values are empty dicts
                is_empty_checks = []
                for val in non_null_values:
                    is_dict = isinstance(val, dict)
                    is_empty = len(val) == 0 if is_dict else False
                    is_empty_checks.append(is_dict and is_empty)
                    if col == 'source_properties':  # Extra logging for problematic column
                        logger.debug(f"[EMPTY_STRUCT_CHECK] source_properties value: type={type(val)}, is_dict={is_dict}, is_empty={is_empty}, repr={repr(val)}")
                
                all_empty_dicts = all(is_empty_checks)
                
                logger.debug(f"[EMPTY_STRUCT_CHECK] Column '{col}': all_empty_dicts={all_empty_dicts}, checks={is_empty_checks[:3]}...")
                
                if all_empty_dicts:
                    logger.info(f"Removing column '{col}' - contains only empty structs")
                    columns_to_drop.append(col)
                    
            except Exception as e:
                logger.warning(f"Error checking column '{col}' for empty structs: {str(e)}")
                import traceback
                logger.warning(f"Traceback: {traceback.format_exc()}")
                continue
        
        # Drop the empty struct columns
        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)
            logger.info(f"Removed {len(columns_to_drop)} empty struct columns: {columns_to_drop}")
        else:
            logger.warning(f"[EMPTY_STRUCT_CHECK] No empty struct columns detected to remove!")
        
        return df
    
    def _clean_event_for_pyarrow(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        DEPRECATED: Use _clean_event_for_awswrangler instead.
        Kept for backward compatibility with any external callers.
        """
        return self._clean_event_for_awswrangler(event)
     
    def _generate_s3_key(self, source_name: str, event_class: str, timestamp: datetime,
                         region: str = 'Azure', account_id: str = 'unknown') -> str:
        """
        Generate S3 key with proper partitioning for Security Lake
        
        Format: {s3_path}/region={region}/accountid={accountid}/eventday=YYYYMMDD/{sourceName}{uuid}.parquet
        
        Args:
            source_name: Source name from configuration
            event_class: OCSF event class name (not used in path, kept for compatibility)
            timestamp: Event timestamp for partitioning
            region: AWS region or 'unknown'
            account_id: Account ID or 'unknown'
            
        Returns:
            Complete S3 key path
        """
        # Generate partition keys
        event_day = timestamp.strftime('%Y%m%d')
        filename = f"{source_name}{uuid4()}.parquet"
        
        # Build S3 key with Security Lake partitioning format
        # Format: ext/.../region=.../accountid=.../eventday=.../source=.../{file}
        parts = [
            self.s3_path,
            f"region={region}",
            f"accountid={account_id}",
            f"eventday={event_day}",
            filename
        ]
        
        # Filter out empty parts and join
        s3_key = '/'.join([p for p in parts if p])
        logger.info(f"Storing parquet file {s3_key}")
        return s3_key